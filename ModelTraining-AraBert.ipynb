{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25e0248f-2f5a-431f-a26a-05710c8acc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Mon Mar 14 19:23:23 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 496.49       Driver Version: 496.49       CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   53C    P3     9W /  N/A |    105MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    !nvidia-smi\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696ebef2-1dbf-4e9f-afef-8ebff3af9d77",
   "metadata": {},
   "source": [
    "# Creating training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5861a525-3c7e-4b59-9fa1-7e85c68a70e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f354c4c3-ac90-40b9-b33b-391faf39ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        train: List[pd.DataFrame],\n",
    "        test: List[pd.DataFrame],\n",
    "        label_list: List[str],\n",
    "    ):\n",
    "        \"\"\"Class to hold and structure datasets.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        name (str): holds the name of the dataset so we can select it later\n",
    "        train (List[pd.DataFrame]): holds training pandas dataframe with 2 columns [\"text\",\"label\"]\n",
    "        test (List[pd.DataFrame]): holds testing pandas dataframe with 2 columns [\"text\",\"label\"]\n",
    "        label_list (List[str]): holds the list  of labels\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.label_list = label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b1bb395-f75b-4da0-ba34-deb738e909ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will hold all the downloaded and structred datasets\n",
    "all_datasets= []\n",
    "DATA_COLUMN = \"text\"\n",
    "LABEL_COLUMN = \"dialect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e3270f-edd6-4d94-bed6-a4e10eedfa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EG    57636\n",
      "PL    43742\n",
      "KW    42109\n",
      "LY    36499\n",
      "QA    31069\n",
      "JO    27921\n",
      "LB    27617\n",
      "SA    26832\n",
      "AE    26296\n",
      "BH    26292\n",
      "OM    19116\n",
      "SY    16242\n",
      "DZ    16183\n",
      "IQ    15497\n",
      "SD    14434\n",
      "MA    11539\n",
      "YE     9927\n",
      "TN     9246\n",
      "Name: dialect, dtype: int64\n",
      "['IQ', 'LY', 'QA', 'PL', 'SY', 'TN', 'JO', 'MA', 'SA', 'YE', 'DZ', 'EG', 'LB', 'KW', 'OM', 'SD', 'AE', 'BH']\n",
      "Training length:  412377\n",
      "Testing length:  45820\n"
     ]
    }
   ],
   "source": [
    "df_dialect = pd.read_csv(\"dialect_dataset_modified.csv\")\n",
    "\n",
    "df_dialect = df_dialect[[\"text\",\"dialect\"]]  # we are interested in text and dialect only\n",
    "df_dialect.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "print(df_dialect[LABEL_COLUMN].value_counts())\n",
    "\n",
    "label_list_dialect = list(df_dialect[LABEL_COLUMN].unique())\n",
    "print(label_list_dialect)\n",
    "\n",
    "train_dialect, test_dialect = train_test_split(df_dialect, test_size=0.1, random_state=42, stratify=df_dialect[LABEL_COLUMN] )\n",
    "\n",
    "print(\"Training length: \", len(train_dialect))\n",
    "print(\"Testing length: \", len(test_dialect))\n",
    "\n",
    "data_dialect = CustomDataset(\"Dialect\", train_dialect, test_dialect, label_list_dialect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f323eb4-a95c-49aa-8678-bdcb01a2f85a",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a064bb27-2653-46ff-87f7-0bf54cc187fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from arabert.preprocess import ArabertPreprocessor, never_split_tokens\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score, precision_score,\n",
    "                             recall_score)\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (AutoConfig, AutoModelForSequenceClassification,\n",
    "                          AutoTokenizer, BertTokenizer, Trainer,\n",
    "                          TrainingArguments)\n",
    "from transformers.data.processors.utils import InputFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a29e0e-f79e-4bf2-b74c-49f9d0069a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"bert-base-arabertv2\" with FarasaSegmenter or use \"bert-base-arabertv02\" for no FarasaSegmenter or use apply_farasa_segmentation=True\n",
    "model_name = 'aubmindlab/bert-base-arabertv02' # we are going to use the twitter AraBERT since it has emojis and dialects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec2efa0a-0543-4d43-b979-abbe72522466",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_prep = ArabertPreprocessor(model_name)\n",
    "\n",
    "data_dialect.train[DATA_COLUMN] = data_dialect.train[DATA_COLUMN].apply(lambda x: arabic_prep.preprocess(x))\n",
    "data_dialect.test[DATA_COLUMN] = data_dialect.test[DATA_COLUMN].apply(lambda x: arabic_prep.preprocess(x))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f1e57b9-883b-46ca-ba34-9b8ba8dd4082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[مستخدم] بل بل بل . . ضاحي سؤال . . متى بتنتحر ؟',\n",
       " '[مستخدم] واايي . . انا افخر والله',\n",
       " '[مستخدم] مايشوف شر # حكيم الخليج',\n",
       " '[مستخدم] قلتلة عنهم يقول وينهم يحسبنهم بالشارع',\n",
       " 'بحس الناس كلها بتنام بالليل صايره ! غريب .',\n",
       " '[مستخدم] مزبوط هدا اللي نقول فيه',\n",
       " '[مستخدم] انا بالله لو سمحت',\n",
       " '[مستخدم] فال انفا بعين دياب',\n",
       " 'اتصدف في # إنستغرام بحسابات ما اذكرني ضفتها ولا اذكرها ضافتني يعني جديدة علي اول مرة اشوفها وهالشي زاد هالفترة بشكل ملحوظ حد واجه نفس الموضوع ؟',\n",
       " '[مستخدم] سما . . رايك انك ماتنتخبش تصبح تبع برك حتى ولو كان قناعة هك ! ! الكلام موجه لصاحب التسمية ! اما انت اختي شكرا على التنوير']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check on the dataset\n",
    "list(data_dialect.train[DATA_COLUMN][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0576597-c476-4b87-88fd-c8dbc62ea33e",
   "metadata": {},
   "source": [
    "Now we need to check the tokenized sentence length to decide on the maximum sentence length value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba727f31-04e0-4d94-bab1-64a4b604a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained(model_name, \n",
    "                                    do_lower_case=False, \n",
    "                                    do_basic_tokenize=True,\n",
    "                                    never_split=never_split_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e29f92a4-a082-4e6a-84a0-d5f4fc64f78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sentence Lengths: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Sentence Lengths: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Training Sentence Lengths: \")\n",
    "plt.hist([ len(tok.tokenize(sentence)) for sentence in data_dialect.train[DATA_COLUMN].to_list()],bins=range(0,128,2))\n",
    "plt.show()\n",
    "\n",
    "print(\"Testing Sentence Lengths: \")\n",
    "plt.hist([ len(tok.tokenize(sentence)) for sentence in data_dialect.test[DATA_COLUMN].to_list()],bins=range(0,128,2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd5c4a0-6512-4bcd-a7ff-91b523916cfb",
   "metadata": {},
   "source": [
    "Let's select 100 as our maximum sentence length, and check how many sequences will be truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b793fba-403a-49c1-a33b-20d612373be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b337f615-7188-4b6b-b29a-3337dda86cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncated training sequences:  8437\n",
      "Truncated testing sequences:  8437\n"
     ]
    }
   ],
   "source": [
    "print(\"Truncated training sequences: \", sum([len(tok.tokenize(sentence)) > max_len for sentence in data_dialect.test[DATA_COLUMN].to_list()]))\n",
    "\n",
    "print(\"Truncated testing sequences: \", sum([len(tok.tokenize(sentence)) > max_len for sentence in data_dialect.test[DATA_COLUMN].to_list()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537996f9-1617-4b79-9910-b885b3f46250",
   "metadata": {},
   "source": [
    "8437 out of ~45000 for testing isn't bad, as to make training faster for 16 batch size to work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3706c0a-9b57-451f-ade8-95a5ea7a9d21",
   "metadata": {},
   "source": [
    "Now let's create a classification dataset to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ad2c537-446f-441c-9a4f-f1defe2f23f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, text, target, model_name, max_len, label_map):\n",
    "        super(ClassificationDataset).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        text (List[str]): List of the training text\n",
    "        target (List[str]): List of the training labels\n",
    "        tokenizer_name (str): The tokenizer name (same as model_name).\n",
    "        max_len (int): Maximum sentence length\n",
    "        label_map (Dict[str,int]): A dictionary that maps the class labels to integer\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.target = target\n",
    "        self.tokenizer_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                                       do_lower_case=False, \n",
    "                                                       do_basic_tokenize=True,\n",
    "                                                       never_split=never_split_tokens)\n",
    "        self.max_len = max_len\n",
    "        self.label_map = label_map\n",
    "      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        text = str(self.text[item])\n",
    "        text = \" \".join(text.split())\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )      \n",
    "        return InputFeatures(**inputs,label=self.label_map[self.target[item]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f75513f-44ba-4d97-8c54-2b12eb49728d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IQ': 0, 'LY': 1, 'QA': 2, 'PL': 3, 'SY': 4, 'TN': 5, 'JO': 6, 'MA': 7, 'SA': 8, 'YE': 9, 'DZ': 10, 'EG': 11, 'LB': 12, 'KW': 13, 'OM': 14, 'SD': 15, 'AE': 16, 'BH': 17}\n"
     ]
    }
   ],
   "source": [
    "label_map = { v:index for index, v in enumerate(data_dialect.label_list) }\n",
    "print(label_map)\n",
    "\n",
    "train_dataset = ClassificationDataset(\n",
    "    data_dialect.train[DATA_COLUMN].to_list(),\n",
    "    data_dialect.train[LABEL_COLUMN].to_list(),\n",
    "    model_name,\n",
    "    max_len,\n",
    "    label_map\n",
    "  )\n",
    "test_dataset = ClassificationDataset(\n",
    "    data_dialect.test[DATA_COLUMN].to_list(),\n",
    "    data_dialect.test[LABEL_COLUMN].to_list(),\n",
    "    model_name,\n",
    "    max_len,\n",
    "    label_map\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5b42f1-5b89-44f8-ad27-461d7681bb8f",
   "metadata": {},
   "source": [
    "Check the dataset output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e3373f1-93ce-4803-a164-5653c03b5213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputFeatures(input_ids=[2, 7, 549, 549, 549, 20, 20, 40798, 3708, 20, 20, 6735, 4017, 58987, 183, 105, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(train_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce1fe21-cea4-4e45-9e0b-77e939efbdf2",
   "metadata": {},
   "source": [
    "Create a function that return a pretrained model ready to do classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "758536cb-4283-47d6-8d29-d4d437c0aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=len(label_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40257c7d-f347-4cd3-92c7-6737debffbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p): #p should be of type EvalPrediction\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    assert len(preds) == len(p.label_ids)\n",
    "    #print(classification_report(p.label_ids,preds))\n",
    "    #print(confusion_matrix(p.label_ids,preds))\n",
    "    macro_f1 = f1_score(p.label_ids,preds,average='macro')\n",
    "    #macro_precision = precision_score(p.label_ids,preds,average='macro')\n",
    "    #macro_recall = recall_score(p.label_ids,preds,average='macro')\n",
    "    acc = accuracy_score(p.label_ids,preds)\n",
    "    return {       \n",
    "      'macro_f1' : macro_f1,\n",
    "      'accuracy': acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa965b11-f8ae-4b96-abc1-db92c4aa2a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=False # turned from True to False\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896e6951-68c8-45c0-9c67-8d47b72f2b4b",
   "metadata": {},
   "source": [
    "# Regular Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea8542-2202-4918-8753-9b7caca152dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fa3bdd4-4e45-45ae-bf67-83ef929d2088",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments( \n",
    "    output_dir= \"./train\",    \n",
    "    adam_epsilon = 1e-8,\n",
    "    learning_rate = 2e-5,\n",
    "    fp16 = True, # enable this when using V100 or T4 GPU\n",
    "    per_device_train_batch_size = 16, # up to 64 on 16GB with max len of 128\n",
    "    per_device_eval_batch_size = 128,\n",
    "    gradient_accumulation_steps = 2, # use this to scale batch size without needing more memory\n",
    "    num_train_epochs= 1,\n",
    "    warmup_ratio = 0,\n",
    "    do_eval = True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True, # this allows to automatically get the best model at the end based on whatever metric we want\n",
    "    metric_for_best_model = 'macro_f1',\n",
    "    greater_is_better = True,\n",
    "    seed = 25\n",
    "  )\n",
    "\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2ff71a-aa0d-469b-9f2f-0a3f8eaca396",
   "metadata": {},
   "source": [
    "Create the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d0d79a6-8481-44f4-bf2e-5937b1756089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model_init(),\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15724258-8fa5-43ca-9f6d-53a265a62340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 412377\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 12887\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12887' max='12887' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12887/12887 49:21, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.479500</td>\n",
       "      <td>1.416772</td>\n",
       "      <td>0.515964</td>\n",
       "      <td>0.549171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 45820\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./train\\checkpoint-12887\n",
      "Configuration saved in ./train\\checkpoint-12887\\config.json\n",
      "Model weights saved in ./train\\checkpoint-12887\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./train\\checkpoint-12887 (score: 0.51596389143443).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12887, training_loss=1.6330110359443504, metrics={'train_runtime': 2963.9778, 'train_samples_per_second': 139.13, 'train_steps_per_second': 4.348, 'total_flos': 6782283415990656.0, 'train_loss': 1.6330110359443504, 'epoch': 1.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start the training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d192bf2-9cf2-4f33-bbe7-73adc5bda2da",
   "metadata": {},
   "source": [
    "Save the model, the tokenizer and the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8df692c6-317f-4b75-8cdf-e1c77c80c6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to output_dir\n",
      "Configuration saved in output_dir\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'IQ', 1: 'LY', 2: 'QA', 3: 'PL', 4: 'SY', 5: 'TN', 6: 'JO', 7: 'MA', 8: 'SA', 9: 'YE', 10: 'DZ', 11: 'EG', 12: 'LB', 13: 'KW', 14: 'OM', 15: 'SD', 16: 'AE', 17: 'BH'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in output_dir\\pytorch_model.bin\n",
      "tokenizer config file saved in output_dir\\tokenizer_config.json\n",
      "Special tokens file saved in output_dir\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('output_dir\\\\tokenizer_config.json',\n",
       " 'output_dir\\\\special_tokens_map.json',\n",
       " 'output_dir\\\\vocab.txt',\n",
       " 'output_dir\\\\added_tokens.json',\n",
       " 'output_dir\\\\tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_label_map = { v:k for k, v in label_map.items()}\n",
    "print(inv_label_map)\n",
    "trainer.model.config.label2id = label_map\n",
    "trainer.model.config.id2label = inv_label_map\n",
    "trainer.save_model(\"output_dir\")\n",
    "train_dataset.tokenizer.save_pretrained(\"output_dir\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
